{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305d11e4-9ffd-43f6-8b0f-9d3af3a8b2f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms.functional as TF\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086af0c3-4564-48fb-8f81-a32c48d6c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_HR_DIR   = \"/Users/nsvnathan/Downloads/div2k/DIV2K_train_HR/DIV2K_train_HR\"\n",
    "VAL_HR_DIR     = \"/Users/nsvnathan/Downloads/div2k/DIV2K_valid_HR/DIV2K_valid_HR\"\n",
    "\n",
    "SCALE_FACTOR   = 2\n",
    "HR_PATCH_SIZE  = 96\n",
    "LR_PATCH_SIZE  = HR_PATCH_SIZE // SCALE_FACTOR\n",
    "\n",
    "BATCH_SIZE     = 4\n",
    "NUM_EPOCHS     = 10\n",
    "LEARNING_RATE  = 1e-4\n",
    "BETA1, BETA2   = 0.9, 0.999\n",
    "\n",
    "NUM_WORKERS    = 0   \n",
    "PIN_MEMORY     = False\n",
    "\n",
    "LAMBDA_CONTENT = 1.0\n",
    "LAMBDA_ADV     = 0.001\n",
    "\n",
    "VERBOSE        = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54402793-2773-46c9-9d6e-e3b277c5341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset_HR_Only(Dataset):\n",
    "    def __init__(self, hr_dir, patch_size=96, scale=4, random_crop=True):\n",
    "        super().__init__()\n",
    "        self.files = sorted(glob.glob(os.path.join(hr_dir, '*.png')))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No PNGs found in {hr_dir}\")\n",
    "        if patch_size % scale != 0:\n",
    "            raise ValueError(\"patch_size must be divisible by scale\")\n",
    "        self.patch = patch_size\n",
    "        self.lr_patch = patch_size // scale\n",
    "        self.random_crop = random_crop\n",
    "        print(f\"Loaded {len(self.files)} images from {hr_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        w, h = img.size\n",
    "\n",
    "        # HR crop or resize\n",
    "        if self.random_crop and w >= self.patch and h >= self.patch:\n",
    "            i0 = random.randint(0, h - self.patch)\n",
    "            j0 = random.randint(0, w - self.patch)\n",
    "            hr = TF.crop(img, i0, j0, self.patch, self.patch)\n",
    "        else:\n",
    "            hr = (TF.center_crop(img, (self.patch, self.patch))\n",
    "                  if w >= self.patch and h >= self.patch\n",
    "                  else TF.resize(img, (self.patch, self.patch),\n",
    "                                 interpolation=InterpolationMode.BICUBIC))\n",
    "\n",
    "        # LR generation\n",
    "        lr = TF.resize(hr, (self.lr_patch, self.lr_patch),\n",
    "                       interpolation=InterpolationMode.BICUBIC)\n",
    "\n",
    "        # To tensor & normalize to [-1,1]\n",
    "        hr_t = TF.to_tensor(hr)\n",
    "        lr_t = TF.to_tensor(lr)\n",
    "        hr_t = TF.normalize(hr_t, [0.5]*3, [0.5]*3)\n",
    "        lr_t = TF.normalize(lr_t, [0.5]*3, [0.5]*3)\n",
    "        return lr_t, hr_t\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    return None if not batch else torch.utils.data.default_collate(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317e7865-fffa-4279-83df-417b44a088aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformer(embed_dim, num_heads, mlp_ratio, dropout):\n",
    "    layer = nn.TransformerEncoderLayer(\n",
    "        d_model=embed_dim,\n",
    "        nhead=num_heads,\n",
    "        dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "        dropout=dropout,\n",
    "        activation='gelu',\n",
    "        batch_first=True\n",
    "    )\n",
    "    return nn.TransformerEncoder(layer, num_layers=6)\n",
    "\n",
    "\n",
    "\n",
    "# TransGAN Generator\n",
    "class TransGenerator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_ch=3, \n",
    "                 embed_dim=256, \n",
    "                 patch_size=2, \n",
    "                 mlp_ratio=4.0, \n",
    "                 num_heads=2, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        # Patch embed: LR_PATCH_SIZE=24 → 12×12 patches if patch_size=2\n",
    "        self.embed = nn.Conv2d(in_ch, embed_dim,\n",
    "                               kernel_size=patch_size,\n",
    "                               stride=patch_size)\n",
    "        num_patches = (LR_PATCH_SIZE // patch_size) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "        self.transformer = make_transformer(embed_dim, num_heads,\n",
    "                                            mlp_ratio, dropout)\n",
    "\n",
    "        # Unpatchify back to feature map\n",
    "        self.depatch = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embed_dim, embed_dim,\n",
    "                               kernel_size=patch_size,\n",
    "                               stride=patch_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Upsample ×4 → ×2 + ×2\n",
    "        ups = []\n",
    "        for _ in range(int(np.log2(SCALE_FACTOR))):\n",
    "            ups += [\n",
    "                nn.Conv2d(embed_dim, embed_dim * 4, 3, padding=1),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "        self.upsample = nn.Sequential(*ups)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(embed_dim, 3, 9, padding=4)\n",
    "        self.tanh     = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,3,24,24]\n",
    "        B = x.size(0)\n",
    "        # patchify + embed → [B, D, H', W']\n",
    "        x = self.embed(x)\n",
    "        # flatten → [B, N, D]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = x + self.pos_embed\n",
    "        # transformer → [B, N, D]\n",
    "        x = self.transformer(x)\n",
    "        # back to [B, D, H', W']\n",
    "        D = x.size(-1)\n",
    "        side = int(np.sqrt(x.size(1)))\n",
    "        x = x.transpose(1, 2).view(B, D, side, side)\n",
    "        x = self.depatch(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv_out(x)\n",
    "        return self.tanh(x)\n",
    "\n",
    "\n",
    "\n",
    "# TransGAN Discriminator\n",
    "class TransDiscriminator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_ch=3, \n",
    "                 embed_dim=256, \n",
    "                 patch_size=4, \n",
    "                 mlp_ratio=4.0, \n",
    "                 num_heads=2, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        # Patch embed for HR patches 96→24\n",
    "        self.embed = nn.Conv2d(in_ch, embed_dim,\n",
    "                               kernel_size=patch_size,\n",
    "                               stride=patch_size)\n",
    "        num_patches = (HR_PATCH_SIZE // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        self.transformer = make_transformer(embed_dim, num_heads,\n",
    "                                            mlp_ratio, dropout)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        # [B, D, 24,24]\n",
    "        x = self.embed(x)\n",
    "        # [B, D, N] → [B, N, D]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # prepend cls token\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        # transformer expects [B, N, D]\n",
    "        x = self.transformer(x)\n",
    "        # take cls token\n",
    "        x = x[:, 0]\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x)\n",
    "        return x  # [B, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a619ab3d-691f-45c7-be3e-a3707cbbec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gen, loader, device):\n",
    "    gen.eval()\n",
    "    tot_psnr = tot_ssim = n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None: \n",
    "                continue\n",
    "            lr, hr = batch\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr = gen(lr)\n",
    "\n",
    "            # first image only\n",
    "            sr_img = ((sr[0].permute(1,2,0).cpu().clamp(-1,1)+1)/2).numpy()\n",
    "            hr_img = ((hr[0].permute(1,2,0).cpu().clamp(-1,1)+1)/2).numpy()\n",
    "\n",
    "            tot_psnr += compare_psnr(hr_img, sr_img, data_range=1)\n",
    "            tot_ssim += compare_ssim(hr_img, sr_img,\n",
    "                                     data_range=1,\n",
    "                                     channel_axis=-1,\n",
    "                                     win_size=7)\n",
    "            n += 1\n",
    "    gen.train()\n",
    "    return (tot_psnr/n, tot_ssim/n) if n else (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2d877a-3fa0-457d-8a9d-6ec04c452ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 800 images from /Users/nsvnathan/Downloads/div2k/DIV2K_train_HR/DIV2K_train_HR\n",
      "Loaded 100 images from /Users/nsvnathan/Downloads/div2k/DIV2K_valid_HR/DIV2K_valid_HR\n",
      "Train batches: 200, Val batches: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████| 200/200 [10:05<00:00,  3.03s/batch, D=0.6547, G=0.1712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  D_loss: 0.6840  G_loss: 0.1947\n",
      " → Val PSNR: 20.0949 dB  SSIM: 0.5093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████| 200/200 [10:07<00:00,  3.04s/batch, D=0.6671, G=0.1063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10  D_loss: 0.6716  G_loss: 0.1221\n",
      " → Val PSNR: 21.7878 dB  SSIM: 0.5568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████| 200/200 [10:10<00:00,  3.05s/batch, D=0.5349, G=0.0893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10  D_loss: 0.6331  G_loss: 0.1074\n",
      " → Val PSNR: 23.6374 dB  SSIM: 0.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████| 200/200 [09:58<00:00,  2.99s/batch, D=0.6065, G=0.0778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10  D_loss: 0.6128  G_loss: 0.0928\n",
      " → Val PSNR: 24.1893 dB  SSIM: 0.6842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████| 200/200 [10:11<00:00,  3.06s/batch, D=0.6264, G=0.1030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10  D_loss: 0.6096  G_loss: 0.0899\n",
      " → Val PSNR: 25.0596 dB  SSIM: 0.7186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████| 200/200 [10:16<00:00,  3.08s/batch, D=0.6538, G=0.0800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10  D_loss: 0.6386  G_loss: 0.0825\n",
      " → Val PSNR: 25.3156 dB  SSIM: 0.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████| 200/200 [10:19<00:00,  3.10s/batch, D=0.6976, G=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10  D_loss: 0.6522  G_loss: 0.0757\n",
      " → Val PSNR: 25.9979 dB  SSIM: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████| 200/200 [10:18<00:00,  3.09s/batch, D=0.7445, G=0.1090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10  D_loss: 0.6592  G_loss: 0.0745\n",
      " → Val PSNR: 25.6842 dB  SSIM: 0.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████| 200/200 [21:34<00:00,  6.47s/batch, D=0.6751, G=0.0777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10  D_loss: 0.6684  G_loss: 0.0734\n",
      " → Val PSNR: 25.7926 dB  SSIM: 0.7671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████| 200/200 [10:16<00:00,  3.08s/batch, D=0.6218, G=0.0641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  D_loss: 0.6730  G_loss: 0.0707\n",
      " → Val PSNR: 26.4782 dB  SSIM: 0.7748\n",
      "\n",
      "Training complete in 115.6 min. Best PSNR: 26.4782 dB\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(\"Using device:\", device)\n",
    "\n",
    "    for d in (TRAIN_HR_DIR, VAL_HR_DIR):\n",
    "        if not os.path.isdir(d):\n",
    "            raise RuntimeError(f\"Directory not found: {d}\")\n",
    "\n",
    "    train_ds = DIV2KDataset_HR_Only(TRAIN_HR_DIR,\n",
    "                                    patch_size=HR_PATCH_SIZE,\n",
    "                                    scale=SCALE_FACTOR,\n",
    "                                    random_crop=True)\n",
    "    val_ds   = DIV2KDataset_HR_Only(VAL_HR_DIR,\n",
    "                                    patch_size=HR_PATCH_SIZE,\n",
    "                                    scale=SCALE_FACTOR,\n",
    "                                    random_crop=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "    G = TransGenerator().to(device)\n",
    "    D = TransDiscriminator().to(device)\n",
    "\n",
    "    content_loss = nn.L1Loss().to(device)\n",
    "    adv_loss     = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    opt_g = optim.Adam(G.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "    opt_d = optim.Adam(D.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "    best_psnr = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        G.train(); D.train()\n",
    "        running_g = running_d = 0.0\n",
    "\n",
    "        loop = train_loader\n",
    "        if VERBOSE:\n",
    "            loop = tqdm(train_loader,\n",
    "                        desc=f\"Epoch {epoch}/{NUM_EPOCHS}\",\n",
    "                        unit=\"batch\")\n",
    "\n",
    "        for batch in loop:\n",
    "            if batch is None: \n",
    "                continue\n",
    "            lr_b, hr_b = batch\n",
    "            lr_b, hr_b = lr_b.to(device), hr_b.to(device)\n",
    "\n",
    "            # Discriminator step\n",
    "            opt_d.zero_grad()\n",
    "            real_out = D(hr_b)\n",
    "            with torch.no_grad():\n",
    "                fake = G(lr_b)\n",
    "            fake_out = D(fake)\n",
    "            real_lbl = torch.ones_like(real_out)\n",
    "            fake_lbl = torch.zeros_like(fake_out)\n",
    "            loss_d = 0.5 * (adv_loss(real_out, real_lbl) +\n",
    "                            adv_loss(fake_out, fake_lbl))\n",
    "            loss_d.backward()\n",
    "            opt_d.step()\n",
    "\n",
    "            # Generator step\n",
    "            opt_g.zero_grad()\n",
    "            sr_b = G(lr_b)\n",
    "            adv_d = adv_loss(D(sr_b), real_lbl)\n",
    "            cont  = content_loss(sr_b, hr_b)\n",
    "            loss_g = LAMBDA_CONTENT * cont + LAMBDA_ADV * adv_d\n",
    "            loss_g.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "            running_d += loss_d.item()\n",
    "            running_g += loss_g.item()\n",
    "\n",
    "            if VERBOSE:\n",
    "                loop.set_postfix(D=f\"{loss_d.item():.4f}\",\n",
    "                                 G=f\"{loss_g.item():.4f}\")\n",
    "\n",
    "    \n",
    "        avg_d = running_d / len(train_loader)\n",
    "        avg_g = running_g / len(train_loader)\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}  D_loss: {avg_d:.4f}  G_loss: {avg_g:.4f}\")\n",
    "\n",
    "     \n",
    "        psnr_val, ssim_val = evaluate(G, val_loader, device)\n",
    "        print(f\" → Val PSNR: {psnr_val:.4f} dB  SSIM: {ssim_val:.4f}\")\n",
    "\n",
    "      \n",
    "        if psnr_val > best_psnr:\n",
    "            best_psnr = psnr_val\n",
    "            torch.save(G.state_dict(), \"transgen_best.pth\")\n",
    "            torch.save(D.state_dict(), \"transdisc_best.pth\")\n",
    "\n",
    "    total_min = (time.time() - start) / 60\n",
    "    print(f\"\\nTraining complete in {total_min:.1f} min. Best PSNR: {best_psnr:.4f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82905fd4-9653-4e8e-a910-b4abd79e95f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
